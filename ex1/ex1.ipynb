{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    %load_ext autoreload\n",
    "    %autoreload 2\n",
    "    %matplotlib inline\n",
    "except NameError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import os.path as osp\n",
    "\n",
    "import torch\n",
    "from sklearn.svm import SVC\n",
    "from fully_connected import FullyConnectedClassifier\n",
    "\n",
    "from cifar_loader import load_cifar_dataset\n",
    "from utils import shuffle_multiple_arrays, fit_and_save, eval_and_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 34\n",
    "save_dir = \"models\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_train_images, orig_train_labels, orig_test_images, orig_test_labels, class_names = (\n",
    "    load_cifar_dataset(\"cifar-10-batches-py\"))\n",
    "train_images, train_labels = shuffle_multiple_arrays(orig_train_images, orig_train_labels, seed=seed)\n",
    "test_images, test_labels = shuffle_multiple_arrays(orig_test_images, orig_test_labels, seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_train: 500   num_test: 100\n"
     ]
    }
   ],
   "source": [
    "X_train = train_images.reshape(len(train_images), -1)\n",
    "X_test = test_images.reshape(len(test_images), -1)\n",
    "y_train = train_labels\n",
    "y_test = test_labels\n",
    "\n",
    "\n",
    "subsample_fraction = 0.01\n",
    "num_train = int(len(X_train) * subsample_fraction)\n",
    "num_test = int(len(X_test) * subsample_fraction)\n",
    "X_train, y_train, X_test, y_test = X_train[:num_train], y_train[:num_train], X_test[:num_test], y_test[:num_test]\n",
    "print(f\"num_train: {num_train}   num_test: {num_test}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = SVC(kernel=\"linear\", random_state=seed)\n",
    "model_name = \"linear_svm\"\n",
    "model_display_name = \"Linear SVM\"\n",
    "\n",
    "fit_and_save(model, X_train, y_train,\n",
    "             save_dir, model_name)\n",
    "\n",
    "eval_and_plot(model, X_test, y_test,\n",
    "              class_names, save_dir, model_name, model_display_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM with RBF Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SVC(kernel=\"rbf\", random_state=seed)\n",
    "model_name = \"rbf_svm\"\n",
    "model_display_name = \"SVM with RBF Kernel\"\n",
    "\n",
    "fit_and_save(model, X_train, y_train,\n",
    "             save_dir, model_name)\n",
    "\n",
    "eval_and_plot(model, X_test, y_test,\n",
    "              class_names, save_dir, model_name, model_display_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Playing around with PyTorch low level api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1000  loss: 3.705   accuracy: 0.45\n",
      "100/1000  loss: 1.832   accuracy: 0.70\n",
      "200/1000  loss: 0.970   accuracy: 0.81\n",
      "300/1000  loss: 0.765   accuracy: 0.90\n",
      "400/1000  loss: 0.710   accuracy: 0.92\n",
      "500/1000  loss: 0.658   accuracy: 0.93\n",
      "600/1000  loss: 0.630   accuracy: 0.93\n",
      "700/1000  loss: 0.619   accuracy: 0.94\n",
      "800/1000  loss: 0.527   accuracy: 0.94\n",
      "900/1000  loss: 0.344   accuracy: 0.96\n",
      "1000/1000  loss: 0.338   accuracy: 0.96\n",
      "\n",
      "my loss implementation VS pytorch loss implementation:\n",
      "pytorch: 0.33822545409202576\n",
      "mine:    0.33822551369667053\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "seed = 34\n",
    "torch.manual_seed(seed)\n",
    "np_state = np.random.RandomState(seed)\n",
    "\n",
    "# create very easy binary classification dataset\n",
    "x0 = np_state.randn(100, 10) * 3\n",
    "x1 = np_state.randn(100, 10) * 7\n",
    "x = np.vstack([x0, x1])\n",
    "y = np.array([0] * len(x0) + [1] * len(x1))\n",
    "perm = np_state.permutation(len(x))\n",
    "x = x[perm]\n",
    "y = y[perm]\n",
    "x = torch.Tensor(x)\n",
    "y = torch.Tensor(y)\n",
    "\n",
    "# create variables for hidden layer and classifier layer\n",
    "W1 = torch.autograd.Variable(torch.randn((10,100)), requires_grad=True)\n",
    "W2 = torch.autograd.Variable(torch.randn((100,1)), requires_grad=True)\n",
    "optimizer = torch.optim.Adam([W1, W2], lr=0.001)\n",
    "\n",
    "iters = 1000\n",
    "for i in range(iters):\n",
    "    # forward pass\n",
    "    h = torch.tanh(torch.matmul(x, W1))\n",
    "#     h = torch.relu(torch.matmul(x, W1))\n",
    "#     h = torch.sigmoid(torch.matmul(x, W1))\n",
    "    p = torch.squeeze(torch.sigmoid(torch.matmul(h, W2)))\n",
    "    \n",
    "    # loss calculation\n",
    "    eps = torch.Tensor([1e-4])\n",
    "    p = torch.min(p, 1-eps)\n",
    "    p = torch.max(p, eps)\n",
    "    loss = - (y * torch.log(p) + (1 - y) * torch.log(1 - p))\n",
    "    loss = torch.mean(loss)\n",
    "    \n",
    "    # report progress\n",
    "    if ((i+1) % (iters//10) == 0) or (i == 0):\n",
    "        accuracy = ((p > 0.5).int() == y).float().mean()\n",
    "        print(f\"{i+1}/{iters}  loss: {loss.item():.3f}   accuracy: {accuracy.item():.2f}\")\n",
    "    \n",
    "    # gradient step\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "print()\n",
    "print(\"my loss implementation VS pytorch loss implementation:\")\n",
    "print(\"pytorch:\", torch.nn.BCELoss()(p, y).item())\n",
    "print(\"mine:   \", loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a fully connected net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1/10   loss: 2.413   accuracy: 0.09\n",
      "epoch 2/10   loss: 2.421   accuracy: 0.12\n",
      "epoch 3/10   loss: 2.363   accuracy: 0.11\n",
      "epoch 4/10   loss: 2.354   accuracy: 0.11\n",
      "epoch 5/10   loss: 2.343   accuracy: 0.12\n",
      "epoch 6/10   loss: 2.358   accuracy: 0.12\n",
      "epoch 7/10   loss: 2.347   accuracy: 0.09\n",
      "epoch 8/10   loss: 2.358   accuracy: 0.11\n",
      "epoch 9/10   loss: 2.344   accuracy: 0.11\n",
      "epoch 10/10   loss: 2.330   accuracy: 0.12\n"
     ]
    }
   ],
   "source": [
    "hidden_size = 256\n",
    "num_hidden_layers = 2\n",
    "activation = \"tanh\"\n",
    "init_type = \"gaussian\"\n",
    "init_gaussian_std = 0.05\n",
    "\n",
    "net = FullyConnectedClassifier(num_classes=len(class_names),\n",
    "                               input_size=X_train.shape[1],\n",
    "                               hidden_size=hidden_size,\n",
    "                               num_hidden_layers=num_hidden_layers,\n",
    "                               activation=activation,\n",
    "                               init_type=init_type,\n",
    "                               init_gaussian_std=init_gaussian_std)\n",
    "\n",
    "optimizer = torch.optim.Adam(net.trainable_params(), lr=0.001)\n",
    "\n",
    "\n",
    "\n",
    "from utils import batchify\n",
    "from losses import cross_entropy_loss\n",
    "\n",
    "\n",
    "epochs = 10\n",
    "batch_size = 32\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    epoch_seed = seed + epoch\n",
    "    batches = batchify(X_train, y_train, batch_size, seed=epoch_seed)\n",
    "    \n",
    "    num_batches = len(batches)\n",
    "    epoch_loss = 0.\n",
    "    for i_batch, (X_batch, y_batch) in enumerate(batches):\n",
    "        # forward\n",
    "        X_batch = torch.FloatTensor(X_batch)\n",
    "        y_batch = torch.LongTensor(y_batch)\n",
    "        probs = net.forward(X_batch)\n",
    "        loss = cross_entropy_loss(probs, y_batch)\n",
    "        \n",
    "        # gradient step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    epoch_loss /= num_batches\n",
    "    \n",
    "    # report progress\n",
    "    pred_labels = net.predict(torch.FloatTensor(X_train)).data.numpy()\n",
    "    accuracy = (pred_labels == y_train).mean()\n",
    "    print(f\"epoch {epoch+1}/{epochs}   loss: {epoch_loss:.3f}   accuracy: {accuracy:.2f}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchenv",
   "language": "python",
   "name": "torchenv"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
